---
title: "hw3_p2_churn"
output: html_document
author: "Brenda Murueta & David Adewole"
date: "2024-02-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This problem is based on one of [Kaggle's Playground Series of competitions](https://www.kaggle.com/docs/competitions). The Playground Series is a nice way to practice building predictive models by "providing interesting and approachable datasets for our community to practice their machine learning skills". 

You do **NOT** need to download any data from Kaggle. I've created a smaller dataset with some other modifications for use in our HW problem. The datafile, `churn.csv`, is available in the `data` subfolder.

This particular [playground dataset involves data about bank customers](https://www.kaggle.com/competitions/playground-series-s4e1) with the target variable being a binary indicator of whether or not the customer left the bank (`Exited`), or "churned". The playground dataset was constructed using another [Kaggle dataset on bank customer churn prediction](https://www.kaggle.com/datasets/shubhammeshram579/bank-customer-churn-prediction). Follow the preceeding link for information about the variables in this dataset. 

This assignment will focus on building simple classification models for
predicting bank customer churn. You'll be doing your work right in this R Markdown document. Feel free to save it first with a modified filename that includes your name. For example, mine would be **hw3_p2_churn_isken.Rmd**.

You'll likely need a bunch of libraries. I've included a few here but you should add any others that you need. If you don't need some of these, feel free to delete such lines.

```{r}
library(dplyr)   # Group by analysis and other SQLish things.
library(ggplot2) # Plotting, of course
library(corrplot) # Correlation plots
library(caret)   # Many aspects of predictive modeling
library(skimr)  # An automated EDA tool 
library(DiagrammeR) # Graph network visualization
library(rpart) # Recursive Partitioning and Regression Trees
library(rpart.plot) # Plot rpart Models
library(RColorBrewer) # Provides colors palettes
library(randomForest)
```
**MAJOR (10%) HACKER EXTRA** Version control

Create a new R Project for this assignment. Put the project under version control with git. Create a private GitHub repository for this project. Use git and GitHub as you go to do commits periodically and push them to your remote repository. After you have completed the assignment and pushed your last commit to your GitHub repo, add me as a Collaborator (my GitHub username is misken) so that I can see your repo.

I cover use of git and GitHub with R Studio in this module on our course web page:

* [http://www.sba.oakland.edu/faculty/isken/courses/mis5470_f23/git_intro.html](http://www.sba.oakland.edu/faculty/isken/courses/mis5470_f23/git_intro.html)

This Hacker Extra is worth 10% of the total number of points in the assignment.

## Step 1: Read in data

Read the `churn.csv` file from the `data` subfolder into a dataframe named `churn`.

```{r read_churn}
churn <- read.csv("./data/churn.csv" , stringsAsFactors = TRUE)
summary(churn)
```

Use `str`, `summary`, and `skim` to get a sense of the data. 

```{r Display the structure of the data}
# Display the structure of the data
str(churn)
```
```{r Display summary statistics using skim()}
# Display summary statistics using skim()
skim(churn)
```

The binary target variable is `Exited` where 1 indicates that the customer left the bank (they "churned"). You'll notice that some of the fields are numeric and some are character data. You might also notice that there are fewer variables in our churn dataset than in the original Kaggle versions.

## Step 2: Factor conversions

Some of the variables clearly should be factors. Change all of the variables to factors that you think should be. Include an explanation of why each of these variables should be converted to factors.

```{r factor_conversions}
# Convert variables to factors
churn$HasCrCard <- as.factor(churn$HasCrCard)
churn$IsActiveMember <- as.factor(churn$IsActiveMember)
churn$Exited <- as.factor(churn$Exited)
#Verify conversion to factors
str(churn)
```

**EXPLANATION** Why each of these variables should be converted to factors?
> In the churn dataset we have already two variable as factors, such as Geography and Gender, therefore there is no need to convert them . In the other han, we have three more variables that should be treated as factors since they represent categorical data with a limited number of discrete categories. Firstable, 'HasCrCard' should be change into factor  because it indiates whether a customer has a credit card or not, with values of 0 or 1, converting it to a factor ensures that is treats it as such. The other variable is 'IsActiveMember', that indicates whether a customer is an active member with values of 1 that means yes and values of 0 that means no. And finally 'Exited'variable that represents if the customer has churned and by converting it to a factor we define it as a categorical variable with two levels: yes=1 and 0=no.  

## Step 3 - Partition into training and test sets

We will use the [caret](https://topepo.github.io/caret/) package to do the partitioning of our data into training and test dataframes. Just run this chunk to create training and test datasets. This way we'll
all be working with the same datasets. Notice that the test set is 20% of
the full dataset.

```{r partition}
# Simple partition into train (80%) and test (20%) set 
set.seed(687) # Do NOT change this
trainIndex <- createDataPartition(churn$Exited, p = .8, 
                                  list = FALSE, 
                                  times = 1)

churn_train <- churn[as.vector(trainIndex), ]  
churn_test <- churn[-as.vector(trainIndex), ]

```
```{r}
# Show the counts for the train and test data frames. 
table(churn_train$Exited)
table(churn_test$Exited)
```

Find the number of customers and the percentage of customers for the two `Exited` levels. You'll
see that there are about 20% of the bank customers exited.

```{r target_prop_check_train}

# Count the number of customers for each Exited level in the training dataset
train_exit_counts <- table(churn_train$Exited)

# Calculate the percentage of customers for each Exited level in the training dataset
train_exit_percentages <- prop.table(train_exit_counts) * 100

# Display the results
train_exit_counts
train_exit_percentages

```


## Step 4: EDA

Do some EDA to try to uncover some relationships that may end up being
useful in building a predictive model for `Exited`. You learned
things in HW2 which should be useful here. You should **ONLY** use `churn_train` for your EDA. You should explore all of the variables.


```{r EDA1 churn_train}
# Use skim() function churn_train dataframe
skim(churn_train)

```
> The skim() function provides a summary of the churn_train dataset, including the mean, standar, number of missing values, the completion rate,the minimum value( p0), the first quantile('p25'), median('p50'), third quantile('p75'), maximum value('p75') and a little histogram indicating the frequency of the values. In this data set we fave 5 factor variables and 6 numeric. 

```{r EDA2 churn_train}
# Explore the relationship between Exited and other variables- Exited vs. Age
ggplot(churn_train, aes(x = Age, fill = Exited)) +
  geom_density(alpha = 0.5) +
  labs(title = "Density Plot of Age by Exited Status")

```
> The density plot displays the shape of the distribution between the 'Age' variable in relation with 'Exited'variable. For Exited=0 the densityy rises around .06 compared to the Exited=1 indicating that are more observations with Exited=0 com[ared tto Exited=1 in the age range. The higest density(peak)for Exited = 0 occurs at age 55, showing that a important proportion of customers who did not exited the bank fall within this age range. For Exited = 1 the highest density (peak) occurs at age 45, meaning tha a significant proportion ofcustomers who exited the bank fall within this range of age. 

```{r EDA3 churn_train}
# Explore the relationship between Exited and other variables- Exited vs. Balance
ggplot(churn_train) + geom_point(aes(x=EstimatedSalary, y=Balance, colour=Exited) ) 
```
> The scatterplot suggests a significant concentration of data points within the ranges of $75,000 to $150,000 for Balance and $50,000 to $200,000 for Estimated Salary. Additionally, the different colors representing the Exited variable indicate that there are more observations with Exited = 0 (customers who did not exit the bank) compared to Exited = 1 (customers who exited the bank). This concentration of data points in the specified ranges for Balance and EstimatedSalary suggests that these are common ranges for these variables among the bank customers in the dataset.


```{r EDA4 churn_train}
#Explore correlations between numerical variables
correlation_matrix <- cor(churn_train[, c("CreditScore", "Age", "Tenure", "Balance", "NumOfProducts", "EstimatedSalary")])
print(correlation_matrix)
corrplot::corrplot(correlation_matrix, method = "number", order= "hclust", bg = "lightblue")

   
```
> This correlation plot shows Age and Balance with a positive correlation coefficient of 0.06.  Balance and NumOfProducts have a negative correlation of -0.36 and CreditScore and NumOfProduct have a positive correlation of 0.012.  

Now that you know a little more about the data, it's time to start building a
few classification models for `Exited`. We will start out using overall prediction accuracy
as our metric but we might want to consider other metrics.

**QUESTION** Why might overall prediction accuracy not be the most appropriate metric to consider? What other metrics might be important and why?

> Overall prediction **accuracy** may not always be the most appropiate when dealing with imbalance datasets, for example in datasets with one class less frequent that the other class, simply predicting the majority class for all instances can result in high accuracy and the models fails to capture the minority class. Other metric that might be important is **sensitivity**, wich measures rhe proportion of actual positive cases that are correctly identified by the model, and **specificity** that measures the proportion of actual negative vases that are correctly identified by the model. 

### Fit a null model

A very simple model would be to simply predict that `Exited` is equal to 0. On
the training data we saw that we'd be ~80% accurate.

Let's create this null model and run a confusion matrix on its "predictions" for both the training and the test data.

```{r tree_null}
# Create a vector of 0's
model_train_null <- rep(0, nrow(churn_train))
model_test_null <- rep(0, nrow(churn_test))

cm_train_null <- caret::confusionMatrix(as.factor(model_train_null), churn_train$Exited, positive = "1")
cm_train_null

cm_test_null <- caret::confusionMatrix(as.factor(model_test_null), churn_test$Exited, positive = "1")
cm_test_null
```

**QUESTION** A few questions:

* Are you surprised that the performance of the null model is almost identical on test and train? Why or why not?
* Explain the sensitivity and specificity values. 

> As we know, the null model serves a baseline for comparison with more complex models and predicts all instances as belonging to the majority class in the datast. So it really helps to understand the performance of the new model. The confusion matrix in both training and testing datasets have high counts of true negatives(TN), with represents instaces correctly identified as negative(class 0), and counts of true positive(TP), wich would represent instances correctly identified as positive (class 1) and this is because the null model predicts all instances as negative, resulting in ncorrect predictions for the majority class(0) and no correct prediction for the minority class(1). 
> **Sensitivity**is also known as the True Positive rate and measures the proportion of actual positive cases(class 1 ) that are correctly identified . In both results, the sensitivity is reported as 0.0 indicating that the model correctly identifies none of the positive cases. This is expected because the model doesn't predict any instances as positice(cass 1), resulting in a true positive count of 0. For **Specificity**, in both results is reported as 1.0, indicating that the model correctly predicted all negatove cases. 

So, as we begin fitting more complicated models, remember that we need to
outperform the null model to make it worth it to use more complicated models.

Now I'm going to ask you to fit three models:

* a logistic regression model
* a simple decision tree
* a random forest

We covered all three of these modeling techniques in the class notes.

For each model type, you should:

* fit the model on the training data,
* assess the model's performance on the training data using the `confusionMatrix` function,
* use the model to make predictions on the test data,
* assess the model's performance on the test data using the `confusionMatrix` function,
* discuss the results

In your discussion of the results you should talk about things like:

* how accurate is the model in predicting on the test data
* is there evidence of overfitting?
* how does the model do in terms of other metrics like sensitivity and specificity
* other things you deem important.

### Fit logistic regression models

You'll start by creating a logistic regression model to predict `Exited`. Since there
are not that many variables, let's use all of them. Here's a code skeleton to help you get started:

**Hint**: There's an easy way to specify your model formula to include all of the predictor variables without typing out all the variable names. 

```{r lr1_train}
# Fit model to training data
 model_lr1 <- glm(Exited ~ ., 
             data=churn_train, family=binomial(link="logit"))
summary(model_lr1)
## Convert fitted model values to fitted classes. Use 0.5 as the
 #threshold for classifying a case as a 1.
class_train_lr1 <- as.factor(ifelse(predict(model_lr1, churn_train, type="response") > 0.5, "1", "0"))
                          
 cm_train_lr1 <- confusionMatrix(class_train_lr1, churn_train$Exited, positive="1")
cm_train_lr1
```

Now, let's predict on test data.

```{r lr1_test}
#Predict on test data
pred_lr1 <- predict(model_lr1, newdata = churn_test, type = "response")
#Convert predicted values to class labels
 class_test_lr1 <- as.factor(ifelse(pred_lr1 > 0.5, "1", "0"))
#Confusion Matrix on test data                        
cm_test_lr1 <- confusionMatrix(class_test_lr1,churn_test$Exited, positive="1")
cm_test_lr1

```

**QUESTION** How did accuracy, sensitivity and specificity change when predicting on test data instead of the training data?

> **Accuracy** on  the train data is 83.63%, and on the test data decireased to 83.20%. **Sensitivity** on the train data is 38.37% and for the test data also decreased to 37.58%. **Specificity** on the train data is 95.61$ and on the test data also decreased to 95.27%. These results suggest that the model performes reasonably good in the training data than in the test data. 

Now change the threshold from 0.5 to 0.4 and create a new model using this new threshold. How does the sensitivity and specificity change as compared to our first logistic regression model? Explain why this happens?

```{r increase_sensitivity}
# Fit model to training data with threshold 0.4
model_lr2 <- glm(Exited ~ ., data = churn_train,family = binomial(link = "logit"))

# Convert fitted model values to fitted classes using threshold 0.4
class_train_lr2 <- ifelse(predict(model_lr2, churn_train, type = "response") > 0.4, "1", "0")
class_train_lr2 <- as.factor(class_train_lr2)

# Confusion matrix for training data
cm_train_lr2 <- confusionMatrix(class_train_lr2, churn_train$Exited, positive = "1")
cm_train_lr2

```

> As a context, the threshold is like a boundary line, if the predicted probability of instances belonging to class 1 is greater than or equal to the threshold, the instance is classified as class 1, and otherwise it woul be classified as class 0. In the first model ( model_lr1 ) **sensitivity** has a value of 38.37% and for the second model (model_lr2) it increases to 50.93%, while **specificity** decreases from 95.61% to 92.12%.  Lowering the threshold to 0.4 means that the model becomes more sensitivity to positive instances but may result in more false positives. 

### Fit simple decision tree model

Now create a simple decision tree model to predict `Exited`. Again,
use all the variables.

```{r tree1_train}
#Decision Tree to predict 'Exited' - using all variables- train data set
model_tree1 <- rpart(Exited ~ ., data=churn_train,method = "class")
model_tree1

# Prediction using the original train data set.
class_train_tree1 <- predict(model_tree1, type="class")
head(predict(model_tree1, type="class"))

#Create a Confusion Matrix - using train data set
cm_train_tree1 <- confusionMatrix(class_train_tree1, churn_train$Exited, positive="1")
cm_train_tree1
```

Create a plot of your decision tree.

```{r decision_tree_plot}
rpart.plot(model_tree1)
```

Explain the bottom left node of your tree. What conditions have to be true for a case to end up being classified by that node? What do those three numbers in the node mean? What does the color of the node mean?

> If a customer's age is less than 43 snd they have 2 to 3 product, the model predicts that they won't exit the bank(Exited = 0). 

```{r rules}
#Confirm/print the rules
rules <- rpart.rules(model_tree1)
print(rules)
```

Now, let's predict on test data.

```{r tree1_test}
# Prediction on test data
pred_tree1 <- predict(model_tree1, newdata = churn_test, type = "class")
#Create Confusion Matrix
cm_test_tree1 <- confusionMatrix(pred_tree1,churn_test$Exited, positive="1")
cm_test_tree1

```

**QUESTION** How does the performance of the decision tree compare to your logistic regression model? 

> Compaaring the data of both Decision Tree and Logistic Regression model, we notice that: The decidion tree has an accuracy of 85.65% while logistic regression model has an accuracy of 83.63%, suggesting that the decision tree model performs better. About sensitivity, the decision tree is 45.41% while logistic regression has 38.37%, agaiin the decision tree performs better at correctly identifying positive cases.And for specificity the decidion tee has 96.29% that is higher that the 95.61% of the logistic regression model, the decidion tree is better at correctly identyfiying negative cases. In overall, The decision tree mofel has a better performance than the logistic regresion model. 

## Fit random forest model

Finally, fit a random forest model.

```{r rf1_train}
# Fit Random Forest model to training data
model_rf_train <- randomForest(Exited ~ ., data = churn_train, mtry=10, importance=TRUE,na.action = na.omit )
print(model_rf_train)
```
```{r}
# Predict classes using the random forest model on the train data
rf_pred_train <- predict(model_rf_train, churn_train)

# Compute the confusion matrix
cm_train_rf <- confusionMatrix(rf_pred_train, churn_train$Exited, positive = "1")

# Print the confusion matrix
print(cm_train_rf)
```
Now, let's predict on test data.

```{r rf1_test}
# Fit Random Forest model to test data
model_rf_test <- randomForest(Exited ~ ., data = churn_test, mtry=10, importance=TRUE,na.action = na.omit )
print(model_rf_test)
```

**QUESTION** Summarize the performance of all three of your models (logistic, tree, random forest)? Is their evidence of overfitting in any of these model and what is your evidence for your answer? Add code chunks as needed.

```{r summarize performance}
lr1.pred <- predict(model_lr1, churn_test, type="response")
tree1.pred <- predict(model_tree1, churn_test, type = "class")
rf.pred <- predict(model_rf_train, churn_test,type = "response")
# Convert predicted values to factors if needed
lr1.pred <- as.factor(ifelse(lr1.pred > 0.5, "1", "0"))
# Confusion matrices
cm_lr <- confusionMatrix(lr1.pred, churn_test$Exited, positive = "1")
cm_tree <- confusionMatrix(tree1.pred, churn_test$Exited, positive = "1")
cm_rf <- confusionMatrix(rf.pred, churn_test$Exited, positive = "1")

sprintf("Model_lr1: Fit acc= %.3f Pred acc = %.3f",cm_train_lr1$overall['Accuracy'], cm_lr$overall['Accuracy'])
sprintf("Model_tree1: Fit acc= %.3f Pred acc = %.3f",cm_train_tree1$overall['Accuracy'], cm_tree$overall['Accuracy'])
sprintf("Model_rf: Fit acc= %.3f Pred acc = %.3f",cm_train_rf $overall['Accuracy'], cm_rf$overall['Accuracy'])
```
> TO determine is there is any evidence of overfitting, we compare the fit accuracy(training data) to its accuracy on prediction.If the fit accuracy is higher than the prediction accuracy it indicates overfitting. In these results, the three models show evidence of overfitting, but the Random forest shows the highest discrepancy. 


**QUESTION** If you had to pick one to use in an actual financial environment, which model would you use and why? As a manager in charge of retention, what model performance metrics are you most interested in? What are the basic tradeoffs you see in terms of the initiatives you might undertake in response to such a model? For example, if you were really interested in reducing the number of customers exiting, maybe there are some things you might do to incent high risk (of exiting) customers to stay. Discuss.

> If we had to choose one model for predicting customer churn in a financial enviroment, we would choose the Logistic Regression, even though Decision Trees have their advantages such as easily read the rules nodes to get easilky decisions,  Logistic Regression would be better in terms of interpretability and efficiency. As a manager , we would choose metrics such as accuracy, sensitivity and specificity to identify custoomers at risk of churn. In order to incentivize high-risk custoomers to stay, we would do several product and service enhacements. Nevertheless, the choosen model and retenntion strategies should be monitored and refined with the business objectives and constraints over the time. 

**HACKER EXTRA**

Create a variable importance plot for your random forest to try to get a sense of which variables are most important in predicting customers likely to churn.Build another random forest using only the top 5 or so variables suggested by the importance plot. How does the performance of this reduced model compare to the original model?

``` {r model_rf_train var impor}
#Variable importance of the Random Forest model- train data
df_imp <- as.data.frame(model_rf_train $importance) %>% 
  arrange(desc(MeanDecreaseAccuracy))
df_imp
```
```{r plot importance}
#Plot Variable importance of the Random Forest model
ggplot(df_imp, aes(x = MeanDecreaseAccuracy, y = reorder(rownames(df_imp), MeanDecreaseAccuracy))) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Mean Decrease in Accuracy", y = "Variable") +
  ggtitle("Variable Importance Plot") +
  theme_minimal()
```


Build another random forest using only the top 5 or so variables
suggested by the importance plot. How does the performance of this reduced model compare to the original model?

```{r Top 5 importance}
#Select top 5 variables
top_variables <- head(rownames(df_imp),5)
top_variables
#Subset the train data with top 5 variables
churn_train_top <- churn_train[,c("Exited",top_variables)]
# Build a new random forest model using only the top variables
new_model_rf_train <- randomForest(Exited ~ ., data =churn_train_top, mtry = 5, importance = TRUE, na.action = na.omit)
#Print model
print(new_model_rf_train)
```
> How does the performance of this reduced model compare to the original model? The performance of the reduce model compared with the original can be evaluated based on the out-of-bag(OOB) error rate and the Confusion Matrix.The OOB estimates how well the model is likely to perform on new, unseen data that was not included. The original model has a OOB error of 14.07% vs 15.40% of the reduced model indicting that the reduced model may not well generalized as well to unseen data as the original model. In other words, the original model is better making accurate predictions on unseen data compared to the reduced model. And for the confusion matrix analysis, we can see that the class.error for predicting positive class(1) is higher in the reduced model(46.48%) compared with the original model(44.81%) indicating that the reduced model is less effective in correctly identifying churned(1)(yes) customer. 

